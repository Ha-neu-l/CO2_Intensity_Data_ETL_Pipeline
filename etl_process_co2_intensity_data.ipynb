{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcPPL7W9GUk2"
      },
      "source": [
        "# **ETL process on Great Britain's Carbon Intensity Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-MFHEePGNwk"
      },
      "source": [
        "This is a basic ETL process using python libraries. The data is extracted from a public API (Carbon Intensity API)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGTlZrTTGbxt"
      },
      "source": [
        "> ***1. Data Acquisition:***\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vRcLpL2JGBkW"
      },
      "outputs": [],
      "source": [
        "#H: import necessary libraries\n",
        "import requests as rq\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3Wxg8l9LORD5"
      },
      "outputs": [],
      "source": [
        "#H: a function that checks the format and validity of a date\n",
        "def verif_date_validity(date):\n",
        "  try:\n",
        "    formatted_date = dt.datetime.strptime(date, '%Y-%m-%d').strftime('%Y-%m-%d') #H: verify date format and validity\n",
        "    return formatted_date\n",
        "  except ValueError:\n",
        "    print(f\"Validation Error: Date '{date}' is not in 'YYYY-MM-DD' format or is not a valid calendar date.\") #H: if not valid return an error msg\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qmR_140YH7nG"
      },
      "outputs": [],
      "source": [
        "#H: defining a function for data extraction with error handling (we get the json data -> put it in a dataframe for future processing, we also handle exception in case of a problem)\n",
        "\n",
        "def get_co2_intensity_data(date): #H: we give the date (YYYY-MM-DD) as an argument to help extracting a specific day data\n",
        "\n",
        "  #----------- H: checking date format (if not valid return an empty df)\n",
        "  validated_date = verif_date_validity(date)\n",
        "  if not validated_date:\n",
        "    print(f\"Extraction Failed: Invalid date format '{date}'.\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  #----------- H: if the date is valid proceed with extraction and return the data\n",
        "  try:\n",
        "\n",
        "    resp = rq.get(f\"https://api.carbonintensity.org.uk/intensity/date/{date}\",timeout=10)\n",
        "    resp.raise_for_status() #H: Raises HTTPError, if one occurred\n",
        "    data = resp.json() #H: put the json returned in a data var\n",
        "\n",
        "    #H: make sure the data is not the error json response of the api\n",
        "    if 'error' in data:\n",
        "      error_code = data['error'].get('code', 'No code')\n",
        "      error_message = data['error'].get('message', 'No specific message.')\n",
        "      print(f\"API returned an error in JSON: Code={error_code}, Message='{error_message}'\")\n",
        "      return pd.DataFrame()\n",
        "\n",
        "    #H: verify the existence of the data\n",
        "    if (data) and ('data' in data) and (len(data['data']) > 0):\n",
        "        co2_intensity_df = pd.DataFrame(data['data'])\n",
        "        print(f\"Successfully extracted {len(co2_intensity_df)} records for {validated_date}.\")\n",
        "        return co2_intensity_df\n",
        "    else:\n",
        "        print(f\"No 'data' records found or unexpected response structure for {validated_date}. Response: {json.dumps(data, indent=2)[:500]}...\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "  #-----------  H: if any exceptin accured during extraction request display the msg of error and return and empty df\n",
        "  except rq.exceptions.ConnectionError as conn_err:\n",
        "    print(f\"Connection Error during extraction for {validated_date}: {conn_err}\")\n",
        "    print(\"\\n Check your internet connection, DNS resolution, or firewall settings.\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  except rq.exceptions.Timeout as timeout_err:\n",
        "    print(f\"Timeout Error during extraction for {validated_date}: {timeout_err}\")\n",
        "    print(\"\\n The API did not respond within the set time. Retry.\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  except rq.exceptions.HTTPError as http_err:\n",
        "    stat_code = resp.status_code\n",
        "    print(f\"HTTP Error during extraction for {validated_date}: {http_err} [Status Code: {stat_code}]\")\n",
        "\n",
        "    if stat_code == 404:\n",
        "      print(\"The requested date might be out of the API's available range.\")\n",
        "    elif stat_code == 429:\n",
        "      print(\"You might have hit the API rate limit. Retry after some time.\")\n",
        "    elif stat_code >= 500:\n",
        "      print(\"Internal server error.\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  except rq.exceptions.RequestException as req_err:\n",
        "    print(f\"There was an ambiguous exception that occurred while handling your request for {validated_date}: {req_err}\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  except json.JSONDecodeError as json_err:\n",
        "    print(f\"Error decoding JSON response during extraction for {validated_date}: {json_err}\")\n",
        "    if resp:\n",
        "        print(f\"\\n The API might have returned non-JSON content. Response start: {resp.text[:200]}...\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"An unhandled error occurred during extraction for {validated_date}: {e}\")\n",
        "    return pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tK1o-jX9b1eM"
      },
      "outputs": [],
      "source": [
        "#H: this one to make sure we get the data of 1 day from (00:00 to 00:00) so we will be doing 2 api calls\n",
        "# For more details about why check the documentation\n",
        "\n",
        "def get_co2_intensity_data_2days(date): #H: this will return 2 Dataframes to be concatenated after in the Cleaning phase\n",
        "  validated_target_date = verif_date_validity(date)\n",
        "  if not validated_target_date:\n",
        "    print(f\"Failed to get raw daily data: Invalid target date format '{date}'.\")\n",
        "    return []\n",
        "\n",
        "  print(f\"\\n--- Data extraction for: {validated_target_date} ---\")\n",
        "\n",
        "  #H: calculate next day's date\n",
        "  start_date = dt.datetime.strptime(validated_target_date, '%Y-%m-%d').date()\n",
        "  next_day = start_date + dt.timedelta(days=1)\n",
        "  next_day_date = next_day.strftime('%Y-%m-%d')\n",
        "\n",
        "  #H: 2 calls of API\n",
        "  df_for_start_day = get_co2_intensity_data(validated_target_date) #H: records of the target day\n",
        "  df_for_next_day = get_co2_intensity_data(next_day_date)     #H: reored of the next day (needed for last hour of target_date)\n",
        "\n",
        "  if (not df_for_start_day.empty) or (not df_for_next_day.empty):\n",
        "      print(f\"Data extraction completed for {validated_target_date} and {next_day_date}.\\n Cleaning will start.\")\n",
        "  else:\n",
        "      print(f\"No data extracted for {validated_target_date} or {next_day_date}. Both DataFrames are empty.\")\n",
        "\n",
        "  return [df_for_start_day, df_for_next_day]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJlCDnntcDMO"
      },
      "source": [
        "> ***2. Data Cleaning:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Laz0XGATcNxS"
      },
      "outputs": [],
      "source": [
        "#H: cleaning and structuring the data\n",
        "def clean_co2_intensity_data(dfs_list, date):\n",
        "\n",
        "  #----------- H: validate the date\n",
        "  validated_date = verif_date_validity(date)\n",
        "  if not validated_date:\n",
        "    print(f\"Cleaning Failed: Invalid target date format '{date}'.\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  print(f\"\\n--- Starting Cleaning Phase for: {date} ---\")\n",
        "\n",
        "  #----------- H: verify the dataframes are not empty first\n",
        "  if not dfs_list or all(df.empty for df in dfs_list):\n",
        "    print(\"No DataFrames provided for cleaning!\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  #----------- H: concatenate the dataframes\n",
        "  combined_df = pd.concat(dfs_list, ignore_index=True)\n",
        "\n",
        "  #----------- H: making sure the from and to are laways datetime objects\n",
        "  combined_df['from'] = pd.to_datetime(combined_df['from'], errors='coerce', utc=True)\n",
        "  combined_df['to'] = pd.to_datetime(combined_df['to'], errors='coerce', utc=True)\n",
        "\n",
        "  #----------- H: remove records with unparsable timestamps\n",
        "  combined_df_len = len(combined_df)\n",
        "  combined_df.dropna(subset=['from', 'to'], inplace=True)\n",
        "  if len(combined_df) < combined_df_len:\n",
        "      print(f\"Informative Message: {combined_df_len - len(combined_df)} rows were removed 'from' or 'to' timestamps.\")\n",
        "  else:\n",
        "      print(\"Informative Message: All 'from'/'to' timestamps parsed successfully.\")\n",
        "\n",
        "\n",
        "  #----------- H: remove duplicated records if exist (since from should be unique we are using it here)\n",
        "  valid_combined_df_len = len(combined_df)\n",
        "  combined_df.drop_duplicates(subset=['from'], inplace=True)\n",
        "  if len(combined_df) < valid_combined_df_len:\n",
        "      print(f\"Informative Message: {valid_combined_df_len - len(combined_df)} duplicate rows removed based on 'from' unicity.\")\n",
        "  else:\n",
        "      print(\"Informative Message: No duplicate rows found based on 'from' timestamp after initial combine.\")\n",
        "\n",
        "  #---------- H: get the day records from target_day 00:00 to next_day 00:00 (this is the main goal that we did 2 calls for)\n",
        "  target_date_obj = dt.datetime.strptime(validated_date, '%Y-%m-%d').date()\n",
        "  target_day_start = dt.datetime.combine(target_date_obj, dt.time.min).replace(tzinfo=dt.timezone.utc)\n",
        "  target_day_end = target_day_start + dt.timedelta(days=1)\n",
        "\n",
        "  filtered_dataFr = combined_df[\n",
        "      (combined_df['from'] >= target_day_start) &\n",
        "      (combined_df['from'] < target_day_end)\n",
        "  ].copy()\n",
        "\n",
        "  if filtered_dataFr.empty:\n",
        "      print(f\"Cleaning aborted: No data found for the full 00:00-23:59:59 period of {validated_date} after filtering.\")\n",
        "      return pd.DataFrame()\n",
        "  else:\n",
        "      print(f\"Data successfully filtered to {len(filtered_dataFr)} records for {validated_date}.\")\n",
        "\n",
        "  #----------- H: working on the composed intensity field\n",
        "\n",
        "  #H: concatenate the intensity_details_df with the original DataFrame and drop the old 'intensity' column\n",
        "  data_without_intensity = filtered_dataFr.drop(columns=['intensity'])\n",
        "  intensity_details_df = pd.DataFrame(index=data_without_intensity.index)\n",
        "  valid_intensity_mask = filtered_dataFr['intensity'].apply(lambda x: isinstance(x, dict))\n",
        "\n",
        "  if valid_intensity_mask.any():\n",
        "    normalized_part = pd.json_normalize(filtered_dataFr.loc[valid_intensity_mask, 'intensity'])\n",
        "    normalized_part.index = filtered_dataFr.loc[valid_intensity_mask].index\n",
        "\n",
        "  #H: rename the columns from the normalized DataFrame to desired names\n",
        "    normalized_part.rename(columns={\n",
        "        'forecast': 'intensity_forecast',\n",
        "        'actual': 'intensity_actual',\n",
        "        'index': 'intensity_index'\n",
        "    }, inplace=True)\n",
        "\n",
        "    intensity_details_df = normalized_part\n",
        "\n",
        "  filtered_dataFr = pd.concat([data_without_intensity, intensity_details_df], axis=1)\n",
        "\n",
        "  #H: convert the values of 'intensity_forecast' and 'intensity_actual' to numerical ones (normally they are numerical but just in case of unexpected events)\n",
        "  filtered_dataFr['intensity_forecast'] = pd.to_numeric(filtered_dataFr['intensity_forecast'], errors='coerce')\n",
        "  filtered_dataFr['intensity_actual'] = pd.to_numeric(filtered_dataFr['intensity_actual'], errors='coerce')\n",
        "\n",
        "  #----------- H: lastly verifying empty fields and duplicates\n",
        "  missing_actual = filtered_dataFr['intensity_actual'].isnull().sum()\n",
        "  if missing_actual > 0:\n",
        "      #H:making 'actual' values as 'forecast' values\n",
        "      filtered_dataFr['intensity_actual'].fillna(filtered_dataFr['intensity_forecast'], inplace=True)\n",
        "      print(f\"Informative Message: Replaced {missing_actual} missing 'intensity_actual' values with 'intensity_forecast' values.\")\n",
        "  else:\n",
        "      print(\"Informative Message: No missing 'intensity_actual' values found.\")\n",
        "\n",
        "  #---------- H: finally returning the cleaned dataframe\n",
        "  print(f\"Cleaning phase complete for {validated_date}. Final record count: {len(filtered_dataFr)}.\")\n",
        "  return filtered_dataFr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYrFpxzo8Kqd"
      },
      "source": [
        "> ***3. Data Transformation:***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6dJUoqqHydg"
      },
      "source": [
        "**Note**: *Before proceeding in this step, I defined my final ideation of my database scheme and I worked in this phase to facilitate the final loading and make my dataframe ready as much as possible.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NGV178lR8UhI"
      },
      "outputs": [],
      "source": [
        "#H: preparing the data to be loaded in a database\n",
        "def transform__co2_intensity_data(cleanedData):\n",
        "  #----------- H: verify the dataframe is not empty first\n",
        "  if (cleanedData.empty):\n",
        "    print(\"No data for transformation!\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  #----------- H: separate date and times\n",
        "  cleanedData['from_date'] = cleanedData['from'].dt.date\n",
        "  cleanedData['from_time'] = cleanedData['from'].dt.time\n",
        "  cleanedData['to_date'] = cleanedData['to'].dt.date\n",
        "  cleanedData['to_time'] = cleanedData['to'].dt.time\n",
        "\n",
        "  #---------- H: calculate the difference between the actual and forcased intensity : forcast_precision\n",
        "  cleanedData['forcast_precision'] = cleanedData['intensity_actual'] - cleanedData['intensity_forecast']\n",
        "\n",
        "  #---------- H: the intensity_index will be changed to a number (for the usage in the intensity_class table)\n",
        "  intensity_index_mapping = {\n",
        "        \"very low\": 1,\n",
        "        \"low\": 2,\n",
        "        \"moderate\": 3,\n",
        "        \"high\": 4,\n",
        "        \"very high\": 5\n",
        "    }\n",
        "  cleanedData['intensity_level_id'] = cleanedData['intensity_index'].map(intensity_index_mapping).astype('Int64')\n",
        "\n",
        "  #---------- H: filter the needed columns only for further processing\n",
        "  transformed_data = cleanedData[['from_date', 'from_time', 'to_date', 'to_time',\n",
        "                                  'intensity_actual', 'intensity_forecast', 'forcast_precision',\n",
        "                                  'intensity_level_id']].copy()\n",
        "\n",
        "  return transformed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttDMvJVhScsy"
      },
      "source": [
        "**Note:** *as I will my database will contain a daily intensity statistics table, I need some extra operations but not be stored in the initial transformed dataframe, but a separated one. And that's what will the following function help in.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-gpwGdtpTkUf"
      },
      "outputs": [],
      "source": [
        "#H: more processing of the data\n",
        "def get_daily_co2_intensity_stats(transformedData):\n",
        "\n",
        "  #----------- H: verify the dataframe is not empty first\n",
        "  if (transformedData.empty):\n",
        "    print(\"No data for anaytics!\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  ##----------- H: else we specify the day date and proceed\n",
        "  day_date = transformedData['from_date'].iloc[0]\n",
        "\n",
        "  #----------- H: get the min and max intesities of a day with the time of eachone\n",
        "  min_intensity_row = transformedData.loc[transformedData['intensity_actual'].idxmin()]\n",
        "  min_intensity = min_intensity_row['intensity_actual']\n",
        "  min_intensity_time = min_intensity_row['from_time']\n",
        "\n",
        "  max_intensity_row = transformedData.loc[transformedData['intensity_actual'].idxmax()]\n",
        "  max_intensity = max_intensity_row['intensity_actual']\n",
        "  max_intensity_time = max_intensity_row['from_time']\n",
        "\n",
        "  #----------- H:  the forcasting precision, intensities (actual nd forecast ) means of a day\n",
        "  precision_mean = transformedData['forcast_precision'].mean()\n",
        "  avg_actual_intensity = transformedData['intensity_actual'].mean()\n",
        "  avg_forecast_intensity = transformedData['intensity_forecast'].mean()\n",
        "\n",
        "  #----------- H: organize the final dataframe\n",
        "  daily_stats_dataFr = pd.DataFrame([{\n",
        "        'date': day_date,\n",
        "        'min_intensity_actual': min_intensity,\n",
        "        'min_intensity_time': min_intensity_time,\n",
        "        'max_intensity_actual': max_intensity,\n",
        "        'max_intensity_time': max_intensity_time,\n",
        "        'mean_forecast_precision': precision_mean,\n",
        "        'avg_actual_intensity': avg_actual_intensity,\n",
        "        'avg_forecast_intensity': avg_forecast_intensity\n",
        "  }])\n",
        "\n",
        "  print(f\" The daily statistics are calculated for {day_date}\")\n",
        "  return daily_stats_dataFr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPzu8J6xNgI3"
      },
      "source": [
        "> ***4. Data Loading:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t0es_2dgNqSY"
      },
      "outputs": [],
      "source": [
        "import sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5Do0LuXBOWGJ"
      },
      "outputs": [],
      "source": [
        "#H: defining the function for db creation\n",
        "def create_database_schema(db_path):\n",
        "\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        print(f\"Creating database schema\")\n",
        "\n",
        "        #-------- H: create and fill intensity_levels table\n",
        "        cursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS intensity_levels (int_level_id INTEGER PRIMARY KEY,int_level_name TEXT UNIQUE NOT NULL)\"\"\")\n",
        "        intensity_levels_data = [\n",
        "            (1, \"very low\"),\n",
        "            (2, \"low\"),\n",
        "            (3, \"moderate\"),\n",
        "            (4, \"high\"),\n",
        "            (5, \"very high\")\n",
        "        ]\n",
        "        cursor.executemany(\"INSERT OR IGNORE INTO intensity_levels (int_level_id, int_level_name) VALUES (?, ?)\", intensity_levels_data)\n",
        "        print(\"intensity_levels table created/filled.\")\n",
        "\n",
        "\n",
        "        #-------- H: create carbon_intensity_records table\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS carbon_intensity_records (\n",
        "                id_record INTEGER PRIMARY KEY,\n",
        "                from_date TEXT NOT NULL,\n",
        "                from_time TEXT NOT NULL,\n",
        "                to_time TEXT NOT NULL,\n",
        "                intensity_actual REAL,\n",
        "                intensity_forecast REAL,\n",
        "                forcast_precision REAL,\n",
        "                intensity_level_id INTEGER,\n",
        "                FOREIGN KEY (intensity_level_id) REFERENCES intensity_levels(int_level_id)\n",
        "            )\n",
        "        \"\"\")\n",
        "        print(\"carbon_intensity_records table created.\")\n",
        "\n",
        "        #-------- H: create carbon_intensity_daily_stats table\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS carbon_intensity_daily_stats (\n",
        "                date TEXT PRIMARY KEY,\n",
        "                min_intensity_actual REAL,\n",
        "                min_intensity_time TEXT,\n",
        "                max_intensity_actual REAL,\n",
        "                max_intensity_time TEXT,\n",
        "                mean_forecast_precision REAL,\n",
        "                avg_actual_intensity REAL,\n",
        "                avg_forecast_intensity REAL\n",
        "            )\n",
        "        \"\"\")\n",
        "        print(\"carbon_intensity_daily_stats table created.\")\n",
        "\n",
        "        conn.commit()\n",
        "        print(\"Informative Message: Database schema creation complete.\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Error: Database schema creation failed: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vCAdfZmgREAS"
      },
      "outputs": [],
      "source": [
        "#H: filling the main table carbon_intensity_records\n",
        "def load_data_to_carbon_intensity_records(data,db_path):\n",
        "\n",
        "  #----------- H: check data availability\n",
        "  if data.empty:\n",
        "    print(\"No data to load.\")\n",
        "    return\n",
        "\n",
        "  conn = None\n",
        "\n",
        "  #------------ H: transformed data loaded to table + hanling errors\n",
        "  try:\n",
        "      conn = sqlite3.connect(db_path)\n",
        "\n",
        "      #H: select columns to match the table schema\n",
        "      data_to_load = data[[\n",
        "          'from_date', 'from_time', 'to_time',\n",
        "          'intensity_actual', 'intensity_forecast', 'forcast_precision',\n",
        "          'intensity_level_id'\n",
        "      ]]\n",
        "\n",
        "      cursor = conn.cursor()\n",
        "      sql_insert = \"\"\"\n",
        "      INSERT OR IGNORE INTO carbon_intensity_records (\n",
        "          from_date, from_time, to_time, intensity_actual, intensity_forecast,\n",
        "          forcast_precision, intensity_level_id\n",
        "      ) VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "      \"\"\"\n",
        "      rows_inserted_count = 0\n",
        "      for index, row in data_to_load.iterrows():\n",
        "        cursor.execute(sql_insert, (\n",
        "              str(row['from_date']),\n",
        "              str(row['from_time']),\n",
        "              str(row['to_time']),\n",
        "              row['intensity_actual'],\n",
        "              row['intensity_forecast'],\n",
        "              row['forcast_precision'],\n",
        "              row['intensity_level_id']\n",
        "          ))\n",
        "        rows_inserted_count += cursor.rowcount\n",
        "\n",
        "      conn.commit()\n",
        "      print(f\"Loaded {rows_inserted_count} new/updated records.\")\n",
        "\n",
        "  except sqlite3.Error as e:\n",
        "      print(f\"Error loading data: {e}\")\n",
        "  finally:\n",
        "      if conn:\n",
        "          conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5h0SeLMdUNJs"
      },
      "outputs": [],
      "source": [
        "#H: filing the carbon_intensity_daily_stats table\n",
        "def load_daily_stats_to_table(data,db_path):\n",
        "\n",
        "  #----------- H: check data availability\n",
        "  if data.empty:\n",
        "        print(\"No data to load.\")\n",
        "        return\n",
        "\n",
        "  conn = None\n",
        "  try:\n",
        "      conn = sqlite3.connect(db_path)\n",
        "      cursor = conn.cursor()\n",
        "\n",
        "      #------------H : create and fill table\n",
        "      sql_upsert = \"\"\"\n",
        "      INSERT OR REPLACE INTO carbon_intensity_daily_stats (\n",
        "          date, min_intensity_actual, min_intensity_time, max_intensity_actual,\n",
        "          max_intensity_time, mean_forecast_precision, avg_actual_intensity,\n",
        "          avg_forecast_intensity\n",
        "      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "      \"\"\"\n",
        "      rows_processed_count = 0\n",
        "      for index, row in data.iterrows():\n",
        "          cursor.execute(sql_upsert, (\n",
        "              str(row['date']),\n",
        "              row['min_intensity_actual'],\n",
        "              str(row['min_intensity_time']),\n",
        "              row['max_intensity_actual'],\n",
        "              str(row['max_intensity_time']),\n",
        "              row['mean_forecast_precision'],\n",
        "              row['avg_actual_intensity'],\n",
        "              row['avg_forecast_intensity']\n",
        "          ))\n",
        "          rows_processed_count += 1\n",
        "\n",
        "      conn.commit()\n",
        "      print(f\"Loaded/updated {rows_processed_count} daily statistics records.\")\n",
        "\n",
        "  except sqlite3.Error as e:\n",
        "      print(f\"Error loading daily statistics data: {e}\")\n",
        "  finally:\n",
        "      if conn:\n",
        "          conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlctCflMVgbI"
      },
      "source": [
        "> ***5. Apply the whole ETL Pipeline on an example:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hieQs71ZauTa",
        "outputId": "e5cdffdf-dfc0-4f65-b2e7-545be120700b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------ Startig the ELT Process -------\n",
            "\n",
            " ____________________| E: EXTRACT |____________________\n",
            "\n",
            " ---------- Extracting data from Carbon Intensity API (Great Britain) -----------------\n",
            "\n",
            "--- Data extraction for: 2023-05-30 ---\n",
            "Successfully extracted 48 records for 2023-05-30.\n",
            "Successfully extracted 48 records for 2023-05-31.\n",
            "Data extraction completed for 2023-05-30 and 2023-05-31.\n",
            " Cleaning will start.\n",
            "\n",
            " ____________________| T: TRANSFORM |____________________\n",
            "\n",
            " ---------- Cleaning the extracted data -----------------\n",
            "\n",
            "--- Starting Cleaning Phase for: 2023-05-30 ---\n",
            "Informative Message: All 'from'/'to' timestamps parsed successfully.\n",
            "Informative Message: No duplicate rows found based on 'from' timestamp after initial combine.\n",
            "Data successfully filtered to 48 records for 2023-05-30.\n",
            "Informative Message: No missing 'intensity_actual' values found.\n",
            "Cleaning phase complete for 2023-05-30. Final record count: 48.\n",
            "\n",
            " ---------- Transforming data -----------------\n",
            "\n",
            " ----------- Setting the daily statistics -------------\n",
            " The daily statistics are calculated for 2023-05-30\n",
            "\n",
            " ____________________| L: LOAD |____________________\n",
            "\n",
            " ---------- Creating the database schema -----------------\n",
            "Creating database schema\n",
            "intensity_levels table created/filled.\n",
            "carbon_intensity_records table created.\n",
            "carbon_intensity_daily_stats table created.\n",
            "Informative Message: Database schema creation complete.\n",
            "\n",
            " ----------- Loading data to tables -------------------\n",
            "Loaded 48 new/updated records.\n",
            "Loaded/updated 1 daily statistics records.\n"
          ]
        }
      ],
      "source": [
        "#H: do the complete process for a specific date\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  #H: defining the required arguments\n",
        "  date = '2023-05-30'\n",
        "  db_directory = '/data'\n",
        "  db_path = db_directory + 'co2_intensity_db.db'\n",
        "\n",
        "\n",
        "  #H: the ELT Process\n",
        "  print(\"------ Startig the ELT Process -------\")\n",
        "\n",
        "  print(\"\\n ____________________| E: EXTRACT |____________________\")\n",
        "  print(\"\\n ---------- Extracting data from Carbon Intensity API (Great Britain) -----------------\")\n",
        "  api_dataFrames_list = get_co2_intensity_data_2days(date)\n",
        "\n",
        "  print(\"\\n ____________________| T: TRANSFORM |____________________\")\n",
        "  print(  \"\\n ---------- Cleaning the extracted data -----------------\")\n",
        "  cleaned_data = clean_co2_intensity_data(api_dataFrames_list, date)\n",
        "\n",
        "  print(  \"\\n ---------- Transforming data -----------------\")\n",
        "  transformed_main_records = transform__co2_intensity_data(cleaned_data)\n",
        "\n",
        "  print( \"\\n ----------- Setting the daily statistics -------------\")\n",
        "  daily_stats = get_daily_co2_intensity_stats(transformed_main_records)\n",
        "\n",
        "  print(\"\\n ____________________| L: LOAD |____________________\")\n",
        "  print(  \"\\n ---------- Creating the database schema -----------------\")\n",
        "  create_database_schema(db_path)\n",
        "\n",
        "  print(\"\\n ----------- Loading data to tables -------------------\")\n",
        "  load_data_to_carbon_intensity_records(transformed_main_records,db_path)\n",
        "  load_daily_stats_to_table(daily_stats,db_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
